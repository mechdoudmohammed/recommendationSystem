{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source : https://www.kaggle.com/datasets/lokeshparab/amazon-products-dataset?resource=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of CSV files found:\n",
      "oldDataset\\Air Conditioners.csv\n",
      "oldDataset\\All Appliances.csv\n",
      "oldDataset\\All Books.csv\n",
      "oldDataset\\All Car and Motorbike Products.csv\n",
      "oldDataset\\All Electronics.csv\n",
      "oldDataset\\All English.csv\n",
      "oldDataset\\All Exercise and Fitness.csv\n",
      "oldDataset\\All Grocery and Gourmet Foods.csv\n",
      "oldDataset\\All Hindi.csv\n",
      "oldDataset\\All Home and Kitchen.csv\n",
      "oldDataset\\All Movies and TV Shows.csv\n",
      "oldDataset\\All Music.csv\n",
      "oldDataset\\All Pet Supplies.csv\n",
      "oldDataset\\All Sports Fitness and Outdoors.csv\n",
      "oldDataset\\All Video Games.csv\n",
      "oldDataset\\Amazon Fashion.csv\n",
      "oldDataset\\Amazon Pharmacy.csv\n",
      "oldDataset\\Amazon-Products.csv\n",
      "oldDataset\\Baby Bath Skin and Grooming.csv\n",
      "oldDataset\\Baby Fashion.csv\n",
      "oldDataset\\Baby Products.csv\n",
      "oldDataset\\Backpacks.csv\n",
      "oldDataset\\Badminton.csv\n",
      "oldDataset\\Bags and Luggage.csv\n",
      "oldDataset\\Ballerinas.csv\n",
      "oldDataset\\Beauty and Grooming.csv\n",
      "oldDataset\\Bedroom Linen.csv\n",
      "oldDataset\\Blu-ray.csv\n",
      "oldDataset\\Camera Accessories.csv\n",
      "oldDataset\\Cameras.csv\n",
      "oldDataset\\Camping and Hiking.csv\n",
      "oldDataset\\Car Accessories.csv\n",
      "oldDataset\\Car and Bike Care.csv\n",
      "oldDataset\\Car Electronics.csv\n",
      "oldDataset\\Car Parts.csv\n",
      "oldDataset\\Cardio Equipment.csv\n",
      "oldDataset\\Casual Shoes.csv\n",
      "oldDataset\\Childrens Books.csv\n",
      "oldDataset\\Clothing.csv\n",
      "oldDataset\\Coffee Tea and Beverages.csv\n",
      "oldDataset\\Cricket.csv\n",
      "oldDataset\\Cycling.csv\n",
      "oldDataset\\Diapers.csv\n",
      "oldDataset\\Diet and Nutrition.csv\n",
      "oldDataset\\Dog supplies.csv\n",
      "oldDataset\\Entertainment Collectibles.csv\n",
      "oldDataset\\Ethnic Wear.csv\n",
      "oldDataset\\Exam Central.csv\n",
      "oldDataset\\Fashion and Silver Jewellery.csv\n",
      "oldDataset\\Fashion Sales and Deals.csv\n",
      "oldDataset\\Fashion Sandals.csv\n",
      "oldDataset\\Fiction Books.csv\n",
      "oldDataset\\Film Songs.csv\n",
      "oldDataset\\Fine Art.csv\n",
      "oldDataset\\Fitness Accessories.csv\n",
      "oldDataset\\Football.csv\n",
      "oldDataset\\Formal Shoes.csv\n",
      "oldDataset\\Furniture.csv\n",
      "oldDataset\\Gaming Accessories.csv\n",
      "oldDataset\\Gaming Consoles.csv\n",
      "oldDataset\\Garden and Outdoors.csv\n",
      "oldDataset\\Gold and Diamond Jewellery.csv\n",
      "oldDataset\\Handbags and Clutches.csv\n",
      "oldDataset\\Headphones.csv\n",
      "oldDataset\\Health and Personal Care.csv\n",
      "oldDataset\\Heating and Cooling Appliances.csv\n",
      "oldDataset\\Home Audio and Theater.csv\n",
      "oldDataset\\Home Dcor.csv\n",
      "oldDataset\\Home Entertainment Systems.csv\n",
      "oldDataset\\Home Furnishing.csv\n",
      "oldDataset\\Home Improvement.csv\n",
      "oldDataset\\Home Storage.csv\n",
      "oldDataset\\Household Supplies.csv\n",
      "oldDataset\\Indian Classical.csv\n",
      "oldDataset\\Indian Language Books.csv\n",
      "oldDataset\\Indoor Lighting.csv\n",
      "oldDataset\\Industrial and Scientific Supplies.csv\n",
      "oldDataset\\Innerwear.csv\n",
      "oldDataset\\International Music.csv\n",
      "oldDataset\\International Toy Store.csv\n",
      "oldDataset\\Janitorial and Sanitation Supplies.csv\n",
      "oldDataset\\Jeans.csv\n",
      "oldDataset\\Jewellery.csv\n",
      "oldDataset\\Kids Clothing.csv\n",
      "oldDataset\\Kids Fashion.csv\n",
      "oldDataset\\Kids Shoes.csv\n",
      "oldDataset\\Kids Watches.csv\n",
      "oldDataset\\Kindle eBooks.csv\n",
      "oldDataset\\Kitchen and Dining.csv\n",
      "oldDataset\\Kitchen and Home Appliances.csv\n",
      "oldDataset\\Kitchen Storage and Containers.csv\n",
      "oldDataset\\Lab and Scientific.csv\n",
      "oldDataset\\Lingerie and Nightwear.csv\n",
      "oldDataset\\Luxury Beauty.csv\n",
      "oldDataset\\Make-up.csv\n",
      "oldDataset\\Mens Fashion.csv\n",
      "oldDataset\\Motorbike Accessories and Parts.csv\n",
      "oldDataset\\Musical Instruments and Professional Audio.csv\n",
      "oldDataset\\Nursing and Feeding.csv\n",
      "oldDataset\\Pantry.csv\n",
      "oldDataset\\PC Games.csv\n",
      "oldDataset\\Personal Care Appliances.csv\n",
      "oldDataset\\Refrigerators.csv\n",
      "oldDataset\\Refurbished and Open Box.csv\n",
      "oldDataset\\Rucksacks.csv\n",
      "oldDataset\\Running.csv\n",
      "oldDataset\\School Bags.csv\n",
      "oldDataset\\School Textbooks.csv\n",
      "oldDataset\\Security Cameras.csv\n",
      "oldDataset\\Sewing and Craft Supplies.csv\n",
      "oldDataset\\Shirts.csv\n",
      "oldDataset\\Shoes.csv\n",
      "oldDataset\\Snack Foods.csv\n",
      "oldDataset\\Speakers.csv\n",
      "oldDataset\\Sports Collectibles.csv\n",
      "oldDataset\\Sports Shoes.csv\n",
      "oldDataset\\Sportswear.csv\n",
      "oldDataset\\STEM Toys Store.csv\n",
      "oldDataset\\Strength Training.csv\n",
      "oldDataset\\Strollers and Prams.csv\n",
      "oldDataset\\Subscribe and Save.csv\n",
      "oldDataset\\Suitcases and Trolley Bags.csv\n",
      "oldDataset\\Sunglasses.csv\n",
      "oldDataset\\T-shirts and Polos.csv\n",
      "oldDataset\\Televisions.csv\n",
      "oldDataset\\Test Measure and Inspect.csv\n",
      "oldDataset\\Textbooks.csv\n",
      "oldDataset\\The Designer Boutique.csv\n",
      "oldDataset\\Toys and Games.csv\n",
      "oldDataset\\Toys Gifting Store.csv\n",
      "oldDataset\\Travel Accessories.csv\n",
      "oldDataset\\Travel Duffles.csv\n",
      "oldDataset\\Value Bazaar.csv\n",
      "oldDataset\\Video Games Deals.csv\n",
      "oldDataset\\Wallets.csv\n",
      "oldDataset\\Washing Machines.csv\n",
      "oldDataset\\Watches.csv\n",
      "oldDataset\\Western Wear.csv\n",
      "oldDataset\\Womens Fashion.csv\n",
      "oldDataset\\Yoga.csv\n",
      "Included 'oldDataset\\Air Conditioners.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\All Appliances.csv' with shape (100, 9).\n",
      "Skipping 'oldDataset\\All Books.csv' because it is empty.\n",
      "Included 'oldDataset\\All Car and Motorbike Products.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\All Electronics.csv' with shape (100, 9).\n",
      "Skipping 'oldDataset\\All English.csv' because it is empty.\n",
      "Included 'oldDataset\\All Exercise and Fitness.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\All Grocery and Gourmet Foods.csv' with shape (100, 9).\n",
      "Skipping 'oldDataset\\All Hindi.csv' because it is empty.\n",
      "Included 'oldDataset\\All Home and Kitchen.csv' with shape (100, 9).\n",
      "Skipping 'oldDataset\\All Movies and TV Shows.csv' because it is empty.\n",
      "Skipping 'oldDataset\\All Music.csv' because it is empty.\n",
      "Included 'oldDataset\\All Pet Supplies.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\All Sports Fitness and Outdoors.csv' with shape (100, 9).\n",
      "Skipping 'oldDataset\\All Video Games.csv' because it is empty.\n",
      "Included 'oldDataset\\Amazon Fashion.csv' with shape (100, 9).\n",
      "Skipping 'oldDataset\\Amazon Pharmacy.csv' because it is empty.\n",
      "Skipping 'oldDataset\\Amazon-Products.csv' due to column mismatch.\n",
      "Included 'oldDataset\\Baby Bath Skin and Grooming.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Baby Fashion.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Baby Products.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Backpacks.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Badminton.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Bags and Luggage.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Ballerinas.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Beauty and Grooming.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Bedroom Linen.csv' with shape (100, 9).\n",
      "Skipping 'oldDataset\\Blu-ray.csv' because it is empty.\n",
      "Included 'oldDataset\\Camera Accessories.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Cameras.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Camping and Hiking.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Car Accessories.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Car and Bike Care.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Car Electronics.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Car Parts.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Cardio Equipment.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Casual Shoes.csv' with shape (100, 9).\n",
      "Skipping 'oldDataset\\Childrens Books.csv' because it is empty.\n",
      "Included 'oldDataset\\Clothing.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Coffee Tea and Beverages.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Cricket.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Cycling.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Diapers.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Diet and Nutrition.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Dog supplies.csv' with shape (100, 9).\n",
      "Skipping 'oldDataset\\Entertainment Collectibles.csv' because it is empty.\n",
      "Included 'oldDataset\\Ethnic Wear.csv' with shape (100, 9).\n",
      "Skipping 'oldDataset\\Exam Central.csv' because it is empty.\n",
      "Included 'oldDataset\\Fashion and Silver Jewellery.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Fashion Sales and Deals.csv' with shape (44, 9).\n",
      "Included 'oldDataset\\Fashion Sandals.csv' with shape (100, 9).\n",
      "Skipping 'oldDataset\\Fiction Books.csv' because it is empty.\n",
      "Skipping 'oldDataset\\Film Songs.csv' because it is empty.\n",
      "Skipping 'oldDataset\\Fine Art.csv' because it is empty.\n",
      "Included 'oldDataset\\Fitness Accessories.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Football.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Formal Shoes.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Furniture.csv' with shape (100, 9).\n",
      "Skipping 'oldDataset\\Gaming Accessories.csv' because it is empty.\n",
      "Skipping 'oldDataset\\Gaming Consoles.csv' because it is empty.\n",
      "Included 'oldDataset\\Garden and Outdoors.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Gold and Diamond Jewellery.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Handbags and Clutches.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Headphones.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Health and Personal Care.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Heating and Cooling Appliances.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Home Audio and Theater.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Home Dcor.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Home Entertainment Systems.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Home Furnishing.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Home Improvement.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Home Storage.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Household Supplies.csv' with shape (100, 9).\n",
      "Skipping 'oldDataset\\Indian Classical.csv' because it is empty.\n",
      "Skipping 'oldDataset\\Indian Language Books.csv' because it is empty.\n",
      "Included 'oldDataset\\Indoor Lighting.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Industrial and Scientific Supplies.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Innerwear.csv' with shape (100, 9).\n",
      "Skipping 'oldDataset\\International Music.csv' because it is empty.\n",
      "Included 'oldDataset\\International Toy Store.csv' with shape (24, 9).\n",
      "Included 'oldDataset\\Janitorial and Sanitation Supplies.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Jeans.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Jewellery.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Kids Clothing.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Kids Fashion.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Kids Shoes.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Kids Watches.csv' with shape (100, 9).\n",
      "Skipping 'oldDataset\\Kindle eBooks.csv' because it is empty.\n",
      "Included 'oldDataset\\Kitchen and Dining.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Kitchen and Home Appliances.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Kitchen Storage and Containers.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Lab and Scientific.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Lingerie and Nightwear.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Luxury Beauty.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Make-up.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Mens Fashion.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Motorbike Accessories and Parts.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Musical Instruments and Professional Audio.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Nursing and Feeding.csv' with shape (100, 9).\n",
      "Skipping 'oldDataset\\Pantry.csv' because it is empty.\n",
      "Skipping 'oldDataset\\PC Games.csv' because it is empty.\n",
      "Included 'oldDataset\\Personal Care Appliances.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Refrigerators.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Refurbished and Open Box.csv' with shape (24, 9).\n",
      "Included 'oldDataset\\Rucksacks.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Running.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\School Bags.csv' with shape (100, 9).\n",
      "Skipping 'oldDataset\\School Textbooks.csv' because it is empty.\n",
      "Included 'oldDataset\\Security Cameras.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Sewing and Craft Supplies.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Shirts.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Shoes.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Snack Foods.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Speakers.csv' with shape (100, 9).\n",
      "Skipping 'oldDataset\\Sports Collectibles.csv' because it is empty.\n",
      "Included 'oldDataset\\Sports Shoes.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Sportswear.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\STEM Toys Store.csv' with shape (48, 9).\n",
      "Included 'oldDataset\\Strength Training.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Strollers and Prams.csv' with shape (100, 9).\n",
      "Skipping 'oldDataset\\Subscribe and Save.csv' because it is empty.\n",
      "Included 'oldDataset\\Suitcases and Trolley Bags.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Sunglasses.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\T-shirts and Polos.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Televisions.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Test Measure and Inspect.csv' with shape (100, 9).\n",
      "Skipping 'oldDataset\\Textbooks.csv' because it is empty.\n",
      "Included 'oldDataset\\The Designer Boutique.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Toys and Games.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Toys Gifting Store.csv' with shape (24, 9).\n",
      "Included 'oldDataset\\Travel Accessories.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Travel Duffles.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Value Bazaar.csv' with shape (66, 9).\n",
      "Skipping 'oldDataset\\Video Games Deals.csv' because it is empty.\n",
      "Included 'oldDataset\\Wallets.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Washing Machines.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Watches.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Western Wear.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Womens Fashion.csv' with shape (100, 9).\n",
      "Included 'oldDataset\\Yoga.csv' with shape (100, 9).\n",
      "\n",
      "All valid CSV files merged successfully. Saved as 'merged_dataset.csv' with shape (10830, 9).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the directory where your .csv files are located\n",
    "directory = 'oldDataset'\n",
    "\n",
    "# Method to get all .csv files in the directory\n",
    "csv_files = glob.glob(f\"{directory}/*.csv\")\n",
    "\n",
    "# Display the names of all .csv files\n",
    "print(\"List of CSV files found:\")\n",
    "for file in csv_files:\n",
    "    print(file)\n",
    "\n",
    "# Define a list to hold DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Define a reference for columns to ensure consistency\n",
    "reference_columns = None\n",
    "\n",
    "# Process each .csv file\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        # Load only the first 100 rows\n",
    "        df = pd.read_csv(file, nrows=100)\n",
    "        \n",
    "        # Skip the file if it's empty\n",
    "        if df.empty:\n",
    "            print(f\"Skipping '{file}' because it is empty.\")\n",
    "            continue\n",
    "        \n",
    "        # Check column consistency\n",
    "        if reference_columns is None:\n",
    "            # Set reference columns based on the first valid file\n",
    "            reference_columns = df.columns.tolist()\n",
    "        else:\n",
    "            # If the current file columns do not match the reference, skip it\n",
    "            if df.columns.tolist() != reference_columns:\n",
    "                print(f\"Skipping '{file}' due to column mismatch.\")\n",
    "                continue\n",
    "        \n",
    "        # Append the valid DataFrame to the list\n",
    "        dataframes.append(df)\n",
    "        print(f\"Included '{file}' with shape {df.shape}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing '{file}': {e}\")\n",
    "\n",
    "# Merge all DataFrames into one if there are any valid files\n",
    "if dataframes:\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    # Save the merged DataFrame to a new CSV file\n",
    "    merged_df.to_csv('merged_dataset.csv', index=False)\n",
    "    print(f\"\\nAll valid CSV files merged successfully. Saved as 'merged_dataset.csv' with shape {merged_df.shape}.\")\n",
    "else:\n",
    "    print(\"No valid CSV files found to merge.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 'merged_dataset.csv' with shape (10830, 9) and columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\n",
      "Warning: Column count mismatch. Expected 6 columns but got 7.\n",
      "Cleaned dataset saved as 'cleaned_merged_dataset.csv' with shape (10830, 7).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the already merged dataset\n",
    "merged_file = 'merged_dataset.csv'\n",
    "\n",
    "try:\n",
    "    # Read the merged dataset\n",
    "    df = pd.read_csv(merged_file)\n",
    "    print(f\"Loaded '{merged_file}' with shape {df.shape} and columns: {df.columns.tolist()}\")\n",
    "\n",
    "    # Step 1: Remove unwanted columns if they exist\n",
    "    columns_to_remove = ['link', 'discount_price']\n",
    "    df = df.drop(columns=columns_to_remove, errors='ignore')\n",
    "    \n",
    "    # Step 2: Define new column names (must match the number of columns after removal)\n",
    "    new_column_names = ['name', 'category', 'sub_category', 'photo', 'no_of_ratings', 'price']\n",
    "    \n",
    "    # Check if the remaining columns match the expected count\n",
    "    if len(df.columns) == len(new_column_names):\n",
    "        df.columns = new_column_names\n",
    "        print(f\"Columns renamed successfully to: {df.columns.tolist()}\")\n",
    "    else:\n",
    "        print(f\"Warning: Column count mismatch. Expected {len(new_column_names)} columns but got {len(df.columns)}.\")\n",
    "\n",
    "    # Step 3: Save the cleaned DataFrame to a new CSV file\n",
    "    cleaned_file = 'cleaned_merged_dataset.csv'\n",
    "    df.to_csv(cleaned_file, index=False)\n",
    "    print(f\"Cleaned dataset saved as '{cleaned_file}' with shape {df.shape}.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{merged_file}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 'cleaned_merged_dataset.csv' with shape (10830, 7)\n",
      "An error occurred: 'photo'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Load the cleaned merged dataset\n",
    "merged_file = 'cleaned_merged_dataset.csv'\n",
    "log_file = 'removed_rows_log.csv'\n",
    "\n",
    "# Function to check if a URL is valid\n",
    "def is_valid_url(url):\n",
    "    try:\n",
    "        response = requests.head(url, timeout=5)  # Use HEAD request for faster checking\n",
    "        return response.status_code == 200\n",
    "    except requests.RequestException:\n",
    "        return False\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv(merged_file)\n",
    "    print(f\"Loaded '{merged_file}' with shape {df.shape}\")\n",
    "\n",
    "    # Step 2: Initialize a list to store removed rows for logging\n",
    "    removed_rows = []\n",
    "\n",
    "    # Step 3: Check each URL in the 'photo' column\n",
    "    for index, row in df.iterrows():\n",
    "        photo_url = row['photo']\n",
    "        if not is_valid_url(photo_url):\n",
    "            # Log the removed row\n",
    "            removed_rows.append(row)\n",
    "            # Drop the row from the DataFrame\n",
    "            df.drop(index, inplace=True)\n",
    "\n",
    "    # Step 4: Save the updated DataFrame (with only valid URLs)\n",
    "    cleaned_file = 'final_cleaned_dataset.csv'\n",
    "    df.to_csv(cleaned_file, index=False)\n",
    "    print(f\"Filtered dataset saved as '{cleaned_file}' with shape {df.shape}.\")\n",
    "\n",
    "    # Step 5: Save the log of removed rows\n",
    "    if removed_rows:\n",
    "        removed_df = pd.DataFrame(removed_rows)\n",
    "        removed_df.to_csv(log_file, index=False)\n",
    "        print(f\"Log of removed rows saved as '{log_file}' with {len(removed_rows)} entries.\")\n",
    "    else:\n",
    "        print(\"No invalid URLs found. No rows removed.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{merged_file}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 'final_cleaned_dataset_with_extra_columns.csv' with shape (10738, 10)\n",
      "Filtered dataset with all columns saved as 'final_cleaned_dataset_with_all_columns_and_discount.csv' with shape (10738, 11).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load the cleaned merged dataset\n",
    "merged_file = 'final_cleaned_dataset_with_extra_columns.csv'\n",
    "\n",
    "# Conversion rate from ₹ to USD (update this as per the current rate)\n",
    "conversion_rate = 0.012  # Example: 1 INR = 0.012 USD\n",
    "\n",
    "# Function to convert ₹ to USD and ensure the result is float\n",
    "def convert_to_usd(price_str):\n",
    "    # Remove the currency symbol and commas\n",
    "    price_str = price_str.replace('₹', '').replace(',', '').strip()\n",
    "    \n",
    "    try:\n",
    "        # Convert the price string to float\n",
    "        price_in_inr = float(price_str)\n",
    "        # Convert INR to USD\n",
    "        price_in_usd = price_in_inr * conversion_rate\n",
    "        return round(price_in_usd, 2)  # Ensure result is a float and round to two decimal places\n",
    "    except ValueError:\n",
    "        # If there's an error in conversion (e.g., empty or invalid value), return None or handle as needed\n",
    "        return None\n",
    "\n",
    "# Function to generate random stock quantity\n",
    "def generate_random_stock():\n",
    "    return random.randint(1, 1000)  # Random stock between 1 and 1000\n",
    "\n",
    "# Function to generate random color\n",
    "def generate_random_color():\n",
    "    colors = ['Red', 'Blue', 'Green', 'Yellow', 'Black', 'White', 'Purple', 'Orange', 'Pink']\n",
    "    return random.choice(colors)  # Randomly choose a color\n",
    "\n",
    "# Function to extract a random number of keywords from the product name\n",
    "def extract_keywords(name):\n",
    "    # Split the name into words\n",
    "    words = name.split()\n",
    "    # Randomly choose between 3 and 5 words\n",
    "    num_keywords = random.randint(3, 5)\n",
    "    # Ensure we don't exceed the number of available words in the name\n",
    "    num_keywords = min(num_keywords, len(words))\n",
    "    \n",
    "    # Randomly select 'num_keywords' words from anywhere in the list\n",
    "    start_index = random.randint(0, len(words) - num_keywords)\n",
    "    return ' '.join(words[start_index:start_index + num_keywords])  # Return the selected keywords\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv(merged_file)\n",
    "    print(f\"Loaded '{merged_file}' with shape {df.shape}\")\n",
    "\n",
    "    # Step 2: Convert the price column from ₹ to USD and ensure it is a float\n",
    "    df['price'] = df['price'].apply(lambda x: convert_to_usd(str(x)))  # Apply the conversion function\n",
    "    \n",
    "    # Remove any rows where the conversion failed (if any)\n",
    "    df = df.dropna(subset=['price'])\n",
    "\n",
    "    # Step 3: Add Random Stock, Color, and Keywords columns\n",
    "    df['stock'] = df['name'].apply(lambda x: generate_random_stock())  # Add random stock\n",
    "    df['color'] = df['name'].apply(lambda x: generate_random_color())  # Add random color\n",
    "    df['keywords'] = df['name'].apply(lambda x: extract_keywords(x))  # Extract random keywords from product name\n",
    "\n",
    "    # Step 4: Add Discount column with random values between 0 and 30\n",
    "    df['discount'] = [random.uniform(0, 30) for _ in range(len(df))]  # Random discount between 0 and 30\n",
    "\n",
    "    # Step 5: Ensure 'ratings' is a float and 'no_of_ratings' is an integer\n",
    "    df['ratings'] = pd.to_numeric(df['ratings'], errors='coerce')  # Convert ratings to float, invalid values become NaN\n",
    "    df['ratings'] = df['ratings'].fillna(0.0)  # Replace NaN with 0.0 (or any appropriate value)\n",
    "    \n",
    "    df['no_of_ratings'] = pd.to_numeric(df['no_of_ratings'], errors='coerce')  # Convert to integer, invalid values become NaN\n",
    "    df['no_of_ratings'] = df['no_of_ratings'].fillna(0).astype(int)  # Replace NaN with 0 and convert to int\n",
    "\n",
    "    # Step 6: Save the updated DataFrame (with converted prices, random stock, color, keywords, and discount)\n",
    "    cleaned_file = 'final_cleaned_dataset_with_all_columns_and_discount.csv'\n",
    "    df.to_csv(cleaned_file, index=False)\n",
    "    print(f\"Filtered dataset with all columns saved as '{cleaned_file}' with shape {df.shape}.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{merged_file}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 'final_cleaned_dataset_with_all_columns_and_discount.csv' with shape (10738, 11)\n",
      "JSON file 'mongo_import_data.json' successfully saved for MongoDB import.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Load the cleaned merged dataset\n",
    "merged_file = 'final_cleaned_dataset_with_all_columns_and_discount.csv'\n",
    "\n",
    "# Function to process and ensure keywords are in an array format\n",
    "def process_keywords(keywords):\n",
    "    if isinstance(keywords, str):\n",
    "        return keywords.split()  # Split the string by spaces into an array of words\n",
    "    elif isinstance(keywords, list):\n",
    "        return keywords  # If already an array, return as is\n",
    "    else:\n",
    "        return []  # If not a string or list, return an empty array\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv(merged_file)\n",
    "    print(f\"Loaded '{merged_file}' with shape {df.shape}\")\n",
    "\n",
    "    # Step 2: Prepare the MongoDB JSON structure\n",
    "    json_data = []\n",
    "    for index, row in df.iterrows():\n",
    "        product = {\n",
    "            \"name\": row['name'],\n",
    "            \"ratings\": row['ratings'],\n",
    "            \"keywords\": process_keywords(row['keywords']),  # Process and ensure keywords is a list of words\n",
    "            \"no_of_ratings\": row['no_of_ratings'],  # Assuming this exists in the dataset\n",
    "            \"description\": row['description'] if 'description' in row else \"\",  # Default empty if no description\n",
    "            \"category\": row['category'],\n",
    "            \"discount\": round(row['discount'], 2) if 'discount' in row and pd.notnull(row['discount']) else 0.0,\n",
    "            \"price\": row['price'],\n",
    "            \"color\": row['color'],  # Treat 'color' as a simple string, not an array\n",
    "            \"stock\": row['stock'] if 'stock' in row else random.randint(1, 1000),  # Random stock if not available\n",
    "            \"createdAt\": datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\"),  # Set createdAt to current timestamp\n",
    "            \"photo\": row['photo']  # Assuming 'photo' column contains correct image path\n",
    "        }\n",
    "        json_data.append(product)\n",
    "\n",
    "    # Step 3: Save the JSON data to a file\n",
    "    json_filename = 'mongo_import_data.json'\n",
    "    with open(json_filename, 'w') as json_file:\n",
    "        json.dump(json_data, json_file, indent=4)\n",
    "    print(f\"JSON file '{json_filename}' successfully saved for MongoDB import.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{merged_file}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct categories saved to distinct_categories.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the dataset\n",
    "csv_file = \"final_cleaned_dataset_with_all_columns_and_discount.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Set photo column to null (None in Python)\n",
    "df[\"photo\"] = None\n",
    "\n",
    "# Extract distinct categories\n",
    "categories = df[\"category\"].unique().tolist()\n",
    "\n",
    "# Create a list of dictionaries with the required format\n",
    "categories_list = [{\"nameCategorie\": category, \"photo\": None} for category in categories]\n",
    "\n",
    "# Save to distinct_categories.json\n",
    "categories_output_file = \"distinct_categories.json\"\n",
    "with open(categories_output_file, \"w\") as f:\n",
    "    json.dump(categories_list, f, indent=4)\n",
    "\n",
    "print(f\"Distinct categories saved to {categories_output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
